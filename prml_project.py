# -*- coding: utf-8 -*-
"""prml_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NjlTSrE2RyepdABF7Ekszm3ib4vLiiYg
"""

from google.colab import drive
drive.mount('/content/drive')

"""#Preprocessing steps"""

import torch #python #keras #tensorflow #pytorch
import torch.nn as nn
from torch.autograd import Variable
import pandas as pd
import pandas as pd
import numpy as np
import matplotlib.pyplot as mp
import sklearn
from sklearn.preprocessing  import LabelEncoder,OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.compose import make_column_transformer
from sklearn.compose import ColumnTransformer
from pandas.core.internals.managers import T

d=pd.read_csv('/content/drive/MyDrive/Pattern Recognition and Machine Learning - 2022 Winter Semester/labs/prml project/healthcare-dataset-stroke-data (1).csv')
d.shape

from google.colab import drive
drive.mount('/content/drive')

#d=pd.read_csv('/content/drive/MyDrive/Pattern Recognition and Machine Learning - 2022 Winter Semester/labs/prml project/healthcare-dataset-stroke-data (1).csv')
#d.shape

#from google.colab import drive
#drive.mount('/content/drive')

d.head()

d['Residence_type'].unique()

d.info()

d.isna().sum()

d['bmi']=d['bmi'].fillna(d['bmi'].mean())
d.isna().sum()

d['stroke'].unique()

d.head()

d["Residence_type"].value_counts()

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

d['gender'].value_counts()

d.columns

"""###Here i removed "id" column"""

g=d.copy()
g.drop(["id"],axis=1,inplace=True)

"""##### Here in gender '1'- male, '0' -female"""

g.head()
#g.columns

g.shape

"""#**EDA**"""

plt.subplot(331)

g['gender'].value_counts().plot(kind='bar', title='Gender ', figsize=(16,9))
plt.xticks(rotation=0)

plt.subplot(332)

g['ever_married'].value_counts().plot(kind='bar', title='married')

plt.xticks(rotation=0)

plt.subplot(333)

g['work_type'].value_counts().plot(kind='bar', title='work_type')

plt.xticks(rotation=0)

plt.subplot(334)

g['Residence_type'].value_counts().plot(kind='bar', title='Residence')

plt.xticks(rotation=0)


plt.subplot(335)

g['smoking_status'].value_counts().plot(kind='bar', title='Smoking status')

plt.xticks(rotation=0)


plt.subplot(336)

g['hypertension'].value_counts().plot(kind='bar', title='Hypertension')

plt.xticks(rotation=0)

plt.subplot(337)

g['heart_disease'].value_counts().plot(kind='bar', title='Heart disease')

plt.xticks(rotation=0)


plt.show()

sns.distplot(g['age'])
plt.title('age')

sns.distplot(g['avg_glucose_level'])
plt.title('avg_glucose_level')

sns.distplot(g['bmi'])
plt.title('bmi')

j=sns.FacetGrid(g, hue="stroke", height=5)
j.map(sns.distplot, "age")
j.add_legend()
plt.show()

j=sns.FacetGrid(g, hue="stroke", height=4)
j.map(sns.distplot, "hypertension")
# map the above form facetgrid with some attributes
j.add_legend()
plt.show()

j=sns.FacetGrid(g, hue="stroke", height=5)
j.map(sns.distplot, "bmi")
j.add_legend()
plt.show()

j=sns.FacetGrid(g, hue="stroke", height=5)
j.map(sns.distplot, "avg_glucose_level")
j.add_legend()
plt.show()

"""comparision of bmi with smoking status, heart disease, ever married"""

j=sns.FacetGrid(g, hue="smoking_status", height=5)
j.map(sns.distplot, "bmi")
j.add_legend()
plt.show()

j=sns.FacetGrid(g, hue="heart_disease", height=5)
j.map(sns.distplot, "bmi")
j.add_legend()
plt.show()

j=sns.FacetGrid(g, hue="ever_married", height=5)
j.map(sns.distplot, "bmi")
j.add_legend()
plt.show()

"""comparision of avg_glucose_level with smoking status, heart disease, ever married"""

j=sns.FacetGrid(g, hue="smoking_status", height=5)
j.map(sns.distplot, "avg_glucose_level")
j.add_legend()
plt.show()

j=sns.FacetGrid(g, hue="heart_disease", height=5)
j.map(sns.distplot, "avg_glucose_level")
j.add_legend()
plt.show()

j=sns.FacetGrid(g, hue="ever_married", height=5)
j.map(sns.distplot, "avg_glucose_level")
j.add_legend()
plt.show()

"""comparision of age with smoking status, heart disease, ever married"""

j=sns.FacetGrid(g, hue="smoking_status", height=5)
j.map(sns.distplot, "age")
j.add_legend()
plt.show()

j=sns.FacetGrid(g, hue="heart_disease", height=5)
j.map(sns.distplot, "age")
j.add_legend()
plt.show()

j=sns.FacetGrid(g, hue="ever_married", height=5)
j.map(sns.distplot, "age")
j.add_legend()
plt.show()

"""###Boxplot"""

#Boxplot to see how the categorical feature "stroke" is distributed with all other four input variables
sns.boxplot(x=g['age'])

sns.boxplot(x=g['avg_glucose_level'])

sns.boxplot(x=g['bmi'])

fig, axes = plt.subplots(2,2, figsize=(16,9))
sns.boxplot( y="bmi", x= "stroke", data=g, orient='v' , ax=axes[0,0])
sns.boxplot( y="avg_glucose_level", x= "stroke", data=g, orient='v' , ax=axes[0, 1])
sns.boxplot( y="age", x= "stroke", data=g, orient='v' , ax=axes[1, 0])
sns.boxplot( y="hypertension", x= "stroke", data=g, orient='v' , ax=axes[1, 1])
plt.show()

"""Heatmap"""

corr = g.corr()
import matplotlib.pyplot as plot
plt.figure(figsize=(12,9))
sns.heatmap(corr, annot=True, square=True)
plt.yticks(rotation=0)
plt.show()

cat_cols = ['gender','ever_married','work_type','Residence_type','smoking_status']

from sklearn.preprocessing import LabelEncoder, OneHotEncoder 
for i in cat_cols:
   l=LabelEncoder()
   g[i]=l.fit_transform(g[i])
   #encoder = OneHotEncoder(sparse=False, handle_unknown='ignore').fit(d[i])
   #encoded_cols = list(encoder.get_feature_names(i))
   #d[i] = encoder.transform(d[i])
g.head()

f= d.drop(['id'], axis = 1)

f.head()

sns.pairplot(f, hue = 'stroke')

"""### Splitting of dataset into train and test data"""

g.shape
g.columns[10]

from sklearn.preprocessing import StandardScaler

#k=g.copy()
#k.drop(["stroke"],axis=1,inplace=True)
from imblearn.over_sampling import SMOTE
x=g.iloc[:,:-1]
y=g.iloc[:,-1]
x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3, random_state = 2002)
sc = StandardScaler()
x_train = sc.fit_transform(x_train, y_train)
x_test = sc.fit_transform(x_test, y_test)
smote = SMOTE()
x_train, y_train = smote.fit_resample(x_train, y_train)
x_test, y_test = smote.fit_resample(x_test, y_test)


print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

y
x.columns

"""Importing modules"""

from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix
from sklearn.metrics import plot_roc_curve, plot_confusion_matrix,classification_report
from sklearn.model_selection import cross_val_score
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score as ac
from sklearn.metrics import precision_score as ps
from sklearn.metrics import recall_score as rs

"""# **FEATURE SELECTION**"""

import pandas as pd
import numpy as np
data =g.copy()
#X = data.iloc[:,0:20]  #independent columns
#y = data.iloc[:,-1]    #target column i.e price range
from sklearn.ensemble import ExtraTreesClassifier
import matplotlib.pyplot as plt
model = ExtraTreesClassifier()
model.fit(x,y)
print(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers
#plot graph of feature importances for better visualization
feat_importances = pd.Series(model.feature_importances_, index=x.column)
feat_importances.nlargest(12).plot(kind='barh')
plt.show()

"""# **1)Logistic Regression**"""

from sklearn.linear_model import LogisticRegression
from sklearn import metrics
#X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)
l1 = LogisticRegression(random_state = 2002)
l1.fit(x_train, y_train)

y_pred1 = l1.predict(x_test)
print(l1.score(x_test, y_test))

from sklearn.metrics import confusion_matrix
confusion_matrix = confusion_matrix(y_test, y_pred1)
print(confusion_matrix)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred1))

plot_roc_curve(l1, x_test, y_test)

"""#**2)Random Forest Classifier**"""

r = RandomForestClassifier(n_jobs=-1, random_state=2002)
r.fit(x_train, y_train)
r.score(x_train, y_train)

r.score(x_test,y_test)

r.get_params()

plot_confusion_matrix(r,x_test,y_test)

# Classification Report

y_pred2 = r.predict(x_test)
print(classification_report(y_test,y_pred2))

plot_roc_curve(r,x_test,y_test)

# 4. Cross Validated Score
from sklearn.model_selection import cross_val_score
score_RFC = cross_val_score(r,x_test,y_test)
score_RFC.T

"""# **3)DECISION TREE CLASSIFIER**"""

clf = DecisionTreeClassifier(random_state = 2022)
clf.fit(x_train, y_train)
y_pred3 = clf.predict(x_test)
accuracy_score(y_test, y_pred3)

print(classification_report(y_test, y_pred3))

plot_roc_curve(clf, x_test, y_test)

"""##**4)SVM**

###Using Linear Kernel
"""

# Fitting SVC Classification to the Training set with linear kernel
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
#r=[1,2,5,0.25,0.5,0.9,9,30]
#for i in r:
    
svc1 = SVC(kernel = 'linear', C=10, random_state = 2002)
svc1.fit(x_train, y_train)
# Predicting the Test set results
y_pred4 = svc1.predict(x_test)
# compute and print accuracy score
print('Model accuracy score with linear kernel and C=10, : {0:0.4f}'. format(accuracy_score(y_test, y_pred4)))
print(y_pred1)

# Classification Report
from sklearn.metrics import classification_report

print(classification_report(y_test,y_pred4))

# Print the Confusion Matrix and slice it into four pieces

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test, y_pred4)
cm

# plot ROC Curve

plot_roc_curve(svc1, x_test, y_test)

"""# **5)KNN**"""

from sklearn.neighbors import KNeighborsClassifier
arr = [5, 7,10,13,15]
for n in arr:
  clf = KNeighborsClassifier(n_neighbors = n)
  clf.fit(x_train, y_train)
  y_pred5 = clf.predict(x_test)
  print("classification report for ",n,' neighbors is: ')
  print(classification_report(y_test, y_pred5))
  print("confusion matrix for ",n,' neighbors is: ')
  print(confusion_matrix(y_test, y_pred5))

clf = KNeighborsClassifier()
clf.fit(x_train, y_train)
y_pred5 = clf.predict(x_test)
plot_roc_curve(clf, x_test, y_test)

"""# **6) XGB**"""

import xgboost as xgb
import warnings
from xgboost import XGBClassifier
warnings.filterwarnings('ignore')
clf =XGBClassifier(max_depth=6,random_state = 2002)
clf.fit(x_train, y_train)
#plot_roc_curve(y_test, y_pred6)
y_pred6 = clf.predict(x_test)
plot_roc_curve(clf,x_test, y_test)
print(classification_report(y_test, y_pred6))

acc=accuracy_score(y_test,y_pred6)
print("accuracy",acc)

"""# **7)Naive Bayes**"""

from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(x_train, y_train)
y_pred7 = clf.predict(x_test)
print(classification_report(y_test, y_pred7))

plot_roc_curve(clf, x_test, y_test)

"""# **comparision of all models**"""

x = ['log_reg','random_forest','dtc','naive bayes','xgb','svm','knn']
from sklearn.metrics import accuracy_score as ac
from sklearn.metrics import precision_score as ps
from sklearn.metrics import recall_score as rs

y1 = [f1_score(y_test, y_pred1),f1_score(y_test, y_pred2),f1_score(y_test, y_pred3),f1_score(y_test, y_pred7),f1_score(y_test, y_pred6),f1_score(y_test, y_pred4),f1_score(y_test, y_pred5)]
y2 = [ac(y_test, y_pred1),ac(y_test, y_pred2),ac(y_test, y_pred3),ac(y_test, y_pred7),ac(y_test, y_pred6),ac(y_test, y_pred4),ac(y_test, y_pred5)]
y3 = [ps(y_test, y_pred1),ps(y_test, y_pred2),ps(y_test, y_pred3),ps(y_test, y_pred7),ps(y_test, y_pred6),ps(y_test, y_pred4),ps(y_test, y_pred5)]
y4 = [rs(y_test, y_pred1),rs(y_test, y_pred2),rs(y_test, y_pred3),rs(y_test, y_pred7),rs(y_test, y_pred6),rs(y_test, y_pred4),rs(y_test, y_pred5)]
y5 = [0.86, 0.89,0.77,0.85,0.87,0.85,0.72]

plt.plot(x,y1,color = 'red', marker = 'o', label = 'f1_scores')
plt.plot(x, y2, color = 'blue', marker = 'o', label = 'accuracies')
plt.plot(x, y3, color = 'green', marker = 'o', label = 'precision')
plt.plot(x, y4, color = 'purple', marker = 'o', label = 'recall')
plt.plot(x, y5, color = 'orange', marker = 'o', label = 'auc')

plt.legend(loc = "best")
plt.ylim(0.5,1.2)

from matplotlib.pyplot import figure
plt.rcParams['figure.figsize'] = [10,10]

plt.show()

"""# **FINAL CHOSEN 4 MODELS: Random Forest Classifier, Decision Tree , Naive Bayes, and XGB**

**DECISION TREE WITH HYPERPARAMETER TUNING**
"""

para = {'max_depth':[4,5,6],'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4,6]}

from sklearn.model_selection import GridSearchCV
g_clf = DecisionTreeClassifier(random_state = 2002)
g_clf = GridSearchCV(
    g_clf, para, verbose=1, cv=3, scoring='accuracy')

gr= g_clf.fit(x_train, y_train)
print('Best Params: ', gr.best_params_)
print('Best Score: ', gr.best_score_)

clf1 = DecisionTreeClassifier(random_state = 2002, max_depth = 6, max_leaf_nodes = 30, min_samples_split = 2)
clf1.fit(x_train, y_train)
y_pred1 = clf1.predict(x_test)
print(classification_report(y_test, y_pred1))

plot_roc_curve(clf1, x_test, y_test)

"""**RANDOM FOREST WITH HYPERPARAMETER TUNING**"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV
clf2 = RandomForestClassifier(random_state = 2002)
random_grid = {'bootstrap': [True, False],
               'max_depth': [3,5,8,10,12, None],
               'max_features': ['auto', 'sqrt'],
               'min_samples_leaf': [1, 2, 4],
               'min_samples_split': [2, 5, 10],
               'n_estimators': [100, 200, 500, 750]}
RSC = RandomizedSearchCV(clf,random_grid,n_jobs=-1)
RSC.fit(x_train,y_train)
RSC.best_params_

clf2 = RandomForestClassifier(random_state = 2002, bootstrap = True, max_depth = None, max_features = 'auto',min_samples_leaf = 1, min_samples_split = 2, n_estimators = 500)
clf2.fit(x_train, y_train)
y_pred2 = clf2.predict(x_test)
print(classification_report(y_test, y_pred2))

rs(y_test, y_pred2)

plot_roc_curve(clf2, x_test, y_test)

"""**NAIVE BAYES WITH HYPERPARAMETER TUNING**"""

from sklearn.naive_bayes import GaussianNB
clf3 = GaussianNB()
clf3.fit(x_train, y_train)
y_pred3 = clf3.predict(x_test)
print(classification_report(y_test, y_pred3))

plot_roc_curve(clf3, x_test, y_test)

"""**XGBoost WITH HYPERPARAMETER TUNING**"""

import xgboost as xgb
import warnings
from xgboost import XGBClassifier
warnings.filterwarnings('ignore')
clf =XGBClassifier()
param_grid = {
    "n_estimators":[500,1000,1500,2000],
    "max_depth":[4,5,6,7],
    'bootstrap':[True,False],
    "learning_rate":[0.05,0.01,0.1]
}

RSC = RandomizedSearchCV(clf,param_grid,n_jobs=-1)
RSC.fit(x_train,y_train)

RSC.best_params_

xg = XGBClassifier(n_estimators=1000,max_depth=7,learning_rate=0.05,bootstrap=False,random_state = 2002)

xg.fit(x_train,y_train)

y_pred4 = xg.predict(x_test)
plot_roc_curve(xg,x_test, y_test)
print(classification_report(y_test, y_pred4))

"""**Comparision graph of all models**"""

x = ['dtc','random_forest','naive bayes','xgb']
from sklearn.metrics import accuracy_score as ac
from sklearn.metrics import precision_score as ps
from sklearn.metrics import recall_score as rs

y1 = [f1_score(y_test, y_pred1),f1_score(y_test, y_pred2),f1_score(y_test, y_pred3),f1_score(y_test, y_pred4)]
y2 = [ac(y_test, y_pred1),ac(y_test, y_pred2),ac(y_test, y_pred3),ac(y_test, y_pred4)]
y3 = [ps(y_test, y_pred1),ps(y_test, y_pred2),ps(y_test, y_pred3),ps(y_test, y_pred4)]
y4 = [rs(y_test, y_pred1),rs(y_test, y_pred2),rs(y_test, y_pred3),rs(y_test, y_pred4)]
y5 = [0.84, 0.87, 0.86, 0.88]

plt.plot(x,y1,color = 'red', marker = 'o', label = 'f1_scores')
plt.plot(x, y2, color = 'blue', marker = 'o', label = 'accuracies')
plt.plot(x, y3, color = 'green', marker = 'o', label = 'precision')
plt.plot(x, y4, color = 'purple', marker = 'o', label = 'recall')
plt.plot(x, y5, color = 'orange', marker = 'o', label = 'auc')

plt.legend(loc = "best")
plt.ylim(0.5,1.2)

from matplotlib.pyplot import figure
plt.rcParams['figure.figsize'] = [10,10]

plt.show()

"""SFFS FOR RANDOM FOREST CLASSIFIER"""

import joblib
import sys
sys.modules['sklearn.externals.joblib'] = joblib
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
sfs = SFS(clf2,k_features = 10,forward = True, floating = True, verbose = 2, scoring = 'f1', cv = 4)
sfs = sfs.fit(x_train, y_train)

import joblib
import sys
sys.modules['sklearn.externals.joblib'] = joblib
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
sfs = SFS(xg,k_features = 10,forward = True, floating = True, verbose = 2, scoring = 'f1', cv = 4)
sfs = sfs.fit(x_train, y_train)